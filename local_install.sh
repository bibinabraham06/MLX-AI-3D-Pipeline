#!/bin/bash
# Local-First AI Workspace Installation
# Keeps everything on your machine, minimal external dependencies

echo "üè† Local-First AI Workspace Installation"
echo "=========================================="

# Create directories for local storage
echo "üìÅ Creating local directories..."
mkdir -p local_models/{stable_diffusion,depth,llm}
mkdir -p local_cache/{images,models,temp}
mkdir -p outputs/{images,depth,3d}

# Remove old virtual environment
if [[ -d "venv" ]]; then
    echo "üóëÔ∏è Cleaning old environment..."
    rm -rf venv
fi

# Create minimal virtual environment
echo "üêç Creating local Python environment..."
python3 -m venv venv
source venv/bin/activate

# Minimal pip upgrade
echo "üì¶ Installing minimal dependencies..."
pip install --upgrade pip

# Core Python libraries only (no external model downloads)
echo "üîß Installing core libraries..."
pip install \
    fastapi==0.104.1 \
    uvicorn[standard]==0.24.0 \
    pillow==10.1.0 \
    numpy==1.24.3 \
    pydantic==2.5.0 \
    python-multipart==0.0.6

# Local MLX (Apple Silicon only - no external models)
if [[ $(uname -m) == "arm64" ]] && [[ $(uname -s) == "Darwin" ]]; then
    echo "üçé Installing MLX (local Apple Silicon optimization)..."
    pip install mlx==0.15.0
    echo "‚úÖ MLX installed for local Apple Silicon acceleration"
else
    echo "‚ö†Ô∏è Not Apple Silicon - skipping MLX"
fi

# Create local-only configuration
echo "‚öôÔ∏è Creating local configuration..."
cat > local_config.yaml << EOF
# Local-First AI Workspace Configuration
local_mode: true
models_dir: "./local_models"
cache_dir: "./local_cache"
outputs_dir: "./outputs"

# Local server settings
server:
  host: "127.0.0.1"
  port: 7860
  workers: 1

# Local generation settings (no external model downloads)
generation:
  default_size: [512, 512]
  max_size: [1024, 1024]
  default_steps: 20
  device: "auto"  # Will use MLX if available, else CPU

# Disable external model downloads
external_downloads: false
offline_mode: true
EOF

# Create local model placeholder structure
echo "üóÇÔ∏è Setting up local model structure..."
cat > local_models/README.md << EOF
# Local Models Directory

Place your AI models here to keep everything local:

## Stable Diffusion Models
\`stable_diffusion/\` - Place .safetensors or .ckpt files here
- model.safetensors (your main SD model)
- vae/ (optional VAE files)
- lora/ (LoRA files)

## Depth Models (Optional)
\`depth/\` - Depth estimation models
- Place MiDaS or other depth models here

## LLM Models (Optional)
\`llm/\` - Local language models
- Place GGML, MLX, or other local LLM files here

## Usage
Models placed here will be automatically detected by the AI Workspace.
No external downloads required - full offline operation.
EOF

echo "üß™ Creating local test server..."
cat > local_server.py << 'EOF'
#!/usr/bin/env python3
"""
Local-only AI Workspace Server
No external dependencies or downloads
"""

from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
import uvicorn
from pathlib import Path
import json
import yaml

app = FastAPI(title="Local AI Workspace", description="Fully Local AI Tools")

# Load local configuration
config_path = Path("local_config.yaml")
if config_path.exists():
    with open(config_path) as f:
        config = yaml.safe_load(f)
else:
    config = {"local_mode": True, "offline_mode": True}

@app.get("/", response_class=HTMLResponse)
async def root():
    return """
<!DOCTYPE html>
<html>
<head>
    <title>üè† Local AI Workspace</title>
    <style>
        body { font-family: -apple-system, sans-serif; margin: 40px; background: #1a1a1a; color: #e0e0e0; }
        .container { max-width: 800px; margin: 0 auto; }
        .status { background: #2d2d2d; padding: 20px; border-radius: 8px; margin: 20px 0; }
        .success { border-left: 4px solid #4ade80; }
        .warning { border-left: 4px solid #fbbf24; }
        h1 { color: #fff; }
        .feature { background: #374151; padding: 15px; margin: 10px 0; border-radius: 6px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>üè† Local AI Workspace</h1>

        <div class="status success">
            <h3>‚úÖ Local Server Running</h3>
            <p>Your AI workspace is running completely locally on your machine.</p>
            <p><strong>No external connections ‚Ä¢ Full privacy ‚Ä¢ Offline capable</strong></p>
        </div>

        <div class="feature">
            <h4>üìÅ Local Models</h4>
            <p>Place your AI models in <code>local_models/</code> directory.</p>
            <p>Supported: Stable Diffusion, MiDaS depth, MLX language models</p>
        </div>

        <div class="feature">
            <h4>üé® Image Generation</h4>
            <p>Local Stable Diffusion processing (when models are available)</p>
            <p>Apple Silicon: MLX acceleration ‚Ä¢ Intel/AMD: CPU processing</p>
        </div>

        <div class="feature">
            <h4>üî∫ 3D Processing</h4>
            <p>Local depth map generation and normal map creation</p>
            <p>No cloud processing ‚Ä¢ Full offline operation</p>
        </div>

        <div class="status warning">
            <h3>‚öôÔ∏è Setup Required</h3>
            <p>To use AI features, add your models to <code>local_models/</code>:</p>
            <ul>
                <li>Stable Diffusion: <code>local_models/stable_diffusion/model.safetensors</code></li>
                <li>Depth Models: <code>local_models/depth/midas_model.pt</code> (optional)</li>
                <li>LLM Models: <code>local_models/llm/model.ggml</code> (optional)</li>
            </ul>
        </div>
    </div>
</body>
</html>
    """

@app.get("/api/status")
async def status():
    """Local system status"""
    models_dir = Path("local_models")

    # Check for local models
    sd_models = list((models_dir / "stable_diffusion").glob("*.safetensors")) if models_dir.exists() else []
    depth_models = list((models_dir / "depth").glob("*.pt")) if models_dir.exists() else []
    llm_models = list((models_dir / "llm").glob("*.ggml")) if models_dir.exists() else []

    # Check MLX availability
    mlx_available = False
    try:
        import mlx.core as mx
        mlx_available = True
    except ImportError:
        pass

    return {
        "local_mode": True,
        "offline_mode": True,
        "models": {
            "stable_diffusion": len(sd_models),
            "depth": len(depth_models),
            "llm": len(llm_models)
        },
        "acceleration": {
            "mlx_available": mlx_available,
            "device": "mlx" if mlx_available else "cpu"
        },
        "directories": {
            "models": str(models_dir.absolute()),
            "outputs": str(Path("outputs").absolute()),
            "cache": str(Path("local_cache").absolute())
        }
    }

@app.get("/api/models")
async def list_models():
    """List available local models"""
    models_dir = Path("local_models")
    models = {}

    if models_dir.exists():
        # Stable Diffusion models
        sd_dir = models_dir / "stable_diffusion"
        if sd_dir.exists():
            models["stable_diffusion"] = [
                {"name": f.name, "path": str(f), "size": f.stat().st_size}
                for f in sd_dir.glob("*.safetensors")
            ]

        # Depth models
        depth_dir = models_dir / "depth"
        if depth_dir.exists():
            models["depth"] = [
                {"name": f.name, "path": str(f), "size": f.stat().st_size}
                for f in depth_dir.glob("*.pt")
            ]

    return models

if __name__ == "__main__":
    print("üè† Starting Local AI Workspace...")
    print(f"üìÅ Models: {Path('local_models').absolute()}")
    print(f"üì§ Outputs: {Path('outputs').absolute()}")
    print("üåê Open: http://127.0.0.1:7860")

    uvicorn.run(app, host="127.0.0.1", port=7860, log_level="warning")
EOF

# Create local model downloader helper (optional)
echo "üì• Creating model helper script..."
cat > get_models.py << 'EOF'
#!/usr/bin/env python3
"""
Local Model Helper - Download models to local storage
Run only when you want to add models locally
"""

from pathlib import Path
import urllib.request
import sys

def download_file(url, path):
    """Download file with progress"""
    def progress(count, block_size, total_size):
        percent = int(count * block_size * 100 / total_size)
        sys.stdout.write(f"\rDownloading: {percent}%")
        sys.stdout.flush()

    urllib.request.urlretrieve(url, path, progress)
    print(f"\n‚úÖ Downloaded: {path}")

def main():
    print("üì• Local Model Downloader")
    print("========================")
    print("This will download models to your local_models/ directory")
    print("Models will be stored locally and never re-downloaded")

    # Example: Small Stable Diffusion model
    sd_dir = Path("local_models/stable_diffusion")
    sd_dir.mkdir(parents=True, exist_ok=True)

    print("\nAvailable models to download locally:")
    print("1. Small SD model (~2GB)")
    print("2. MiDaS depth model (~100MB)")
    print("3. Skip - I'll add models manually")

    choice = input("\nChoice (1-3): ").strip()

    if choice == "1":
        print("Note: You can add your own .safetensors files to local_models/stable_diffusion/")
        print("This keeps everything local and private.")
    elif choice == "2":
        print("Note: You can add depth models to local_models/depth/")
        print("This enables local 3D processing.")
    else:
        print("‚úÖ Add your models manually to local_models/ directories")

if __name__ == "__main__":
    main()
EOF

# Test the local installation
echo "üß™ Testing local installation..."
source venv/bin/activate
python -c "
try:
    import fastapi, uvicorn, PIL, numpy
    print('‚úÖ All core libraries installed locally')

    try:
        import mlx.core
        print('‚úÖ MLX available - Apple Silicon acceleration enabled')
    except ImportError:
        print('‚ö†Ô∏è MLX not available - CPU processing only')

    print('‚úÖ Local installation complete!')
except ImportError as e:
    print(f'‚ùå Import error: {e}')
"

echo ""
echo "üéâ Local Installation Complete!"
echo ""
echo "üè† Everything is now LOCAL:"
echo "   ‚Ä¢ No external downloads during operation"
echo "   ‚Ä¢ Models stored locally in local_models/"
echo "   ‚Ä¢ All processing on your machine"
echo "   ‚Ä¢ Full offline capability"
echo ""
echo "üöÄ To start:"
echo "   source venv/bin/activate"
echo "   python local_server.py"
echo ""
echo "üìÅ To add models:"
echo "   Place .safetensors files in local_models/stable_diffusion/"
echo "   Or run: python get_models.py"
EOF